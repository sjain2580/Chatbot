import asyncio
import time
from typing import Dict, Any, List, Optional
import httpx
import os
import logging
from dotenv import load_dotenv

# Configure basic logging for better visibility in the backend console
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables from the .env file in the backend directory
load_dotenv()

class AIService:
    """
    Manages asynchronous communication with the Hugging Face Inference API, 
    implementing fallback models for improved reliability.
    """
    def __init__(self):
        # The API key MUST be set as an environment variable (e.g., in .env or deployment host)
        self.api_key = os.getenv("HUGGINGFACE_API_KEY")
        
        if not self.api_key:
            # Critical error: fail fast if the secret is missing.
            raise EnvironmentError(
                "HUGGINGFACE_API_KEY environment variable is not set. "
                "Please set it before running the application."
            )
            
        # Define models with descriptive names (better than indexes)
        self.models = [
            "microsoft/DialoGPT-medium",
            "facebook/blenderbot-400M-distill",
            "gpt2"
        ]
        
        # Use a single shared client instance for potential connection pooling/efficiency
        # Note: httpx.AsyncClient should ideally be managed at the application startup/shutdown level (e.g., in FastAPI/Flask-Async)
        # For simplicity within this class, we instantiate it here, though we use `async with` blocks inside `_call_model`.
        self.base_url = "https://api-inference.huggingface.co/models"
        self.headers = {"Authorization": f"Bearer {self.api_key}"}

    async def _call_model(self, model_url: str, payload: Dict[str, Any], client: httpx.AsyncClient) -> Optional[str]:
        """
        [SRP] Responsibility: Makes the actual API request and handles HTTP status codes.
        Returns the raw generated text or None if the call failed/model is loading.
        """
        api_endpoint = f"{self.base_url}/{model_url}"
        
        try:
            response = await client.post(
                api_endpoint,
                headers=self.headers,
                json=payload,
                timeout=30.0  # Increased timeout for potentially slow loading models
            )

            if response.status_code == 200:
                data = response.json()
                # Assuming the primary model response structure is a list of dicts with 'generated_text'
                if isinstance(data, list) and data and isinstance(data[0], dict):
                    return data[0].get('generated_text', '')
                
                # Handle single string responses or other non-standard formats
                return str(data)

            elif response.status_code == 503:
                # 503 Service Unavailable often means the model is loading
                logger.warning(f"Model {model_url} is currently loading (503).")
                return None  # Signal to try the next model

            else:
                logger.error(f"API Error ({model_url}): Status {response.status_code}. Response: {response.text[:150]}")
                return None  # Signal failure, try next model

        except httpx.RequestError as e:
            logger.error(f"Connection error with model {model_url}: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error processing response from {model_url}: {e}")
            return None

    def _clean_response(self, content: str, original_message: str) -> str:
        """
        [SRP] Responsibility: Cleans up the raw text generated by the model.
        """
        # 1. Remove the user's prompt if the model echoes it
        if content.startswith(original_message):
            content = content[len(original_message):].strip()
        
        # 2. Basic cleanup (remove leading/trailing whitespace, unwanted characters)
        content = content.strip()
        
        # 3. Ensure the response is adequate
        if not content or len(content) < 10:
            return f"I understand you asked: '{original_message}'. The AI response was too short or empty. Please try phrasing your question differently."
        
        return content


    async def generate_response(self, message: str, conversation_history: List[str] = None) -> Dict[str, Any]:
        """
        [SRP] Responsibility: Manages the fallback strategy and final result assembly.
        """
        start_time = time.time()
        
        # Prepare the conversation payload (basic, can be enhanced with history later)
        payload = {"inputs": message}
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            for model_url in self.models:
                
                generated_text = await self._call_model(model_url, payload, client)
                
                if generated_text is not None:
                    # Successfully received a response, now clean and return
                    content = self._clean_response(generated_text, message)
                    
                    end_time = time.time()
                    response_time = int((end_time - start_time) * 1000)

                    # Successful response structure
                    return {
                        "response": content,
                        "response_time": response_time,
                        "token_count": len(content.split()),
                        "model": model_url
                    }
                
        # If the loop completes without returning, all models failed
        end_time = time.time()
        response_time = int((end_time - start_time) * 1000)
        
        return {
            "response": f"I received your message: '{message}'. This is a demo deployment using free-tier services, and all AI models failed to respond. Please try again or check the developer's notes.",
            "response_time": response_time,
            "token_count": 50,
            "model": "failure_fallback"
        }
    
    async def health_check(self) -> bool:
        """
        Simple health check for the API service.
        """
        return True
